### ğŸ“ TextCleaner - Limpieza y NormalizaciÃ³n de Texto en Spark NLP

#### ğŸ“Œ DescripciÃ³n
`TextCleaner` es un pipeline de procesamiento de texto basado en **Spark NLP** que aplica varias transformaciones para limpiar, normalizar y optimizar texto en tareas de NLP.

---

#### ğŸ”„ Proceso de Limpieza
1. **ExpansiÃ³n de Contracciones**  
   - Convierte contracciones a su forma extendida.  
   - Ejemplo: `"can't"` â†’ `"cannot"`, `"I'm"` â†’ `"I am"`.

2. **ConversiÃ³n a MinÃºsculas**  
   - Asegura la uniformidad del texto.

3. **TokenizaciÃ³n**  
   - Divide el texto en tokens individuales.

4. **Filtrado de Stopwords**  
   - Elimina palabras irrelevantes para NLP, **pero conserva negaciones importantes** como:  
     `"not"`, `"cannot"`, `"never"`, `"without"`, `"don't"`, etc.

5. **NormalizaciÃ³n de Texto**  
   - Corrige caracteres especiales, acentos y signos de puntuaciÃ³n.

6. **LematizaciÃ³n**  
   - Convierte palabras a su forma base.  
   - Ejemplo: `"running"` â†’ `"run"`, `"better"` â†’ `"good"`.

7. **GeneraciÃ³n de Embeddings** *(opcional)*  
   - Puede generar representaciones vectoriales para modelos de ML/NLP.

---

#### ğŸ“¥ Entrada
Un **DataFrame de Spark** con una columna de texto.

#### ğŸ“¤ Salida
Un **DataFrame de Spark** con columnas adicionales:
- `document`: RepresentaciÃ³n estructurada del texto.
- `tokens`: Lista de palabras tokenizadas.
- `normalized`: Texto limpio y normalizado.
- `filtered_tokens`: Tokens sin stopwords innecesarias.
- `lemmatized`: Tokens en su forma base.

---

#### ğŸš€ Ejemplo de Uso
```python
cleaner = TextCleaner(spark, use_lemma=True, use_stop_words=True, expand_contractions=True)
cleaned_df = cleaner.clean_dataframe(df)
cleaned_df.select("lemmatized").show(truncate=False)
