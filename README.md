### ğŸ“ TextCleaner - Limpieza y NormalizaciÃ³n de Texto en Spark NLP

#### ğŸ“Œ DescripciÃ³n
`TextCleaner` es un pipeline de procesamiento de texto basado en **Spark NLP** que aplica varias transformaciones para limpiar, normalizar y optimizar texto en tareas de NLP.

---

#### ğŸ”„ Proceso de Limpieza
1. **ExpansiÃ³n de Contracciones**  
   - Convierte contracciones a su forma extendida.  
   - Ejemplo: `"can't"` â†’ `"cannot"`, `"I'm"` â†’ `"I am"`.

2. **ConversiÃ³n a MinÃºsculas**  
   - Asegura la uniformidad del texto.

3. **TokenizaciÃ³n**  
   - Divide el texto en tokens individuales.

4. **Filtrado de Stopwords**  
   - Elimina palabras irrelevantes para NLP, **pero conserva negaciones importantes** como:  
     `"not"`, `"cannot"`, `"never"`, `"without"`, `"don't"`, etc.

5. **NormalizaciÃ³n de Texto**  
   - Corrige caracteres especiales, acentos y signos de puntuaciÃ³n.

6. **LematizaciÃ³n**  
   - Convierte palabras a su forma base.  
   - Ejemplo: `"running"` â†’ `"run"`, `"better"` â†’ `"good"`.

7. **GeneraciÃ³n de Embeddings** *(opcional)*  
   - Puede generar representaciones vectoriales para modelos de ML/NLP.

---

#### ğŸ“¥ Entrada
Un **DataFrame de Spark** con una columna de texto.

#### ğŸ“¤ Salida
Un **DataFrame de Spark** con columnas adicionales:
- `document`: RepresentaciÃ³n estructurada del texto.
- `tokens`: Lista de palabras tokenizadas.
- `normalized`: Texto limpio y normalizado.
- `filtered_tokens`: Tokens sin stopwords innecesarias.
- `lemmatized`: Tokens en su forma base.

---

#### ğŸš€ Ejemplo de Uso
```python
cleaner = TextCleaner(spark, use_lemma=True, use_stop_words=True, expand_contractions=True)
cleaned_df = cleaner.clean_dataframe(df)
cleaned_df.select("lemmatized").show(truncate=False)
```

###  Flujo de Entrenamiento con Spark NLP y Optuna ğŸ¯

Este documento describe el proceso de entrenamiento utilizando **Spark NLP** para la limpieza y procesamiento de datos de texto, junto con **Optuna** para la optimizaciÃ³n de hiperparÃ¡metros en un modelo de clasificaciÃ³n basado en **BERT** y **TF-IDF**.

 ğŸ“Œ 1. InicializaciÃ³n del Entorno
Se inicia la sesiÃ³n de **Spark NLP** para procesar datos de texto:

```python
import sparknlp
spark = sparknlp.start()
processor = SparkNLPProcessor(spark_session=spark)
spark = processor.get_session()
```

#### ğŸ“Œ 2. Carga y Preprocesamiento de Datos
Se carga el dataset desde un archivo CSV y se filtran los datos para incluir solo los ejemplos con `sentiment` igual a `0` o `1`. Luego, se aplica una limpieza de texto avanzada.

```python
datasete = DatasetLoader(spark)
test = datasete.load_csv('/content/drive/MyDrive/NeoNexus/train_dataset.csv')
df_filtered = test.filter(col("sentiment").isin([0, 1]))
df_ = TextCleaner(spark, use_lemma=True, use_stop_words=True, expand_contractions=True).clean_dataframe(df_filtered)
```

#### ğŸ“Œ 3. Carga de Representaciones de Texto
Se cargan las representaciones de texto preprocesadas con **BERT** y **TF-IDF**, junto con las etiquetas de clasificaciÃ³n.

```python
import numpy as np
bert_numpy = np.load("/content/drive/MyDrive/NeoNexus/bert_numpy.npy")
tfidf_numpy = np.load("/content/drive/MyDrive/NeoNexus/tfidf_features.npy")
labels = np.load("/content/drive/MyDrive/NeoNexus/labels.npy")
```

#### ğŸ“Œ 4. OptimizaciÃ³n de HiperparÃ¡metros con Optuna
Se define una clase `HyperparameterOptimization` que emplea **Optuna** para explorar combinaciones de hiperparÃ¡metros y seleccionar la mejor configuraciÃ³n para el modelo.

```python
study = HyperparameterOptimization(
    bert_numpy=bert_numpy,
    tfidf_numpy=tfidf_numpy,
    labels=labels
).optimize()
```

La optimizaciÃ³n se realiza en funciÃ³n de:
- **Tasa de aprendizaje (`lr`)**
- **Decaimiento de peso (`weight_decay`)**
- **Dropout (`dropout`)**
- **TamaÃ±o del batch (`batch_size`)**
- **Optimizador (`Adam`, `AdamW`, `SGD`)**

#### ğŸ“Œ 5. Entrenamiento del Modelo
El modelo es entrenado utilizando la mejor configuraciÃ³n de hiperparÃ¡metros obtenida por **Optuna**.

```python
trainer = Trainer(
    input_tfidf_dim=tfidf_numpy.shape[1],
    input_bert_dim=bert_numpy.shape[1],
    bert_numpy=bert_numpy,
    tfidf_numpy=tfidf_numpy,
    labels=labels,
    criterion=nn.CrossEntropyLoss(),
    optimizer=selected_optimizer,
    epochs=35,
    batch_size=batch_size,
    learning_rate=lr,
    weight_decay=weight_decay,
    dropout=dropout
)
val_losses = trainer.train()
```

---


