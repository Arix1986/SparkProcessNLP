# Frontend - AnÃ¡lisis de Sentimientos de Twitter

Video:  [Link a Google Drive](https://drive.google.com/file/d/1ksrJAg3GBKwjYtjWS7p-z5UxODcmP9Sn/view?usp=sharing)

Este repositorio contiene el cÃ³digo del frontend y backend para un sistema de anÃ¡lisis de sentimientos de tweets. El frontend estÃ¡ desarrollado con **Streamlit** y el backend con **Flask**. El backend se comunica con un scraper de Twitter para obtener tweets y realizar un anÃ¡lisis de sentimientos bÃ¡sico.

## Estructura del Proyecto

- **`frontend.py`**: AplicaciÃ³n de Streamlit que permite al usuario ingresar parÃ¡metros para buscar tweets y visualizar los resultados del anÃ¡lisis de sentimientos.
- **`backend.py`**: Servidor Flask que recibe las solicitudes del frontend, ejecuta el scraper de Twitter y realiza un anÃ¡lisis de sentimientos bÃ¡sico.

## Requisitos

Para ejecutar este proyecto, necesitas tener instalado lo siguiente:

- Python 3.8 o superior.
- Las bibliotecas de Python listadas en `requirements.txt`.

### InstalaciÃ³n de Dependencias

1. Clona este repositorio.
2. Navega a la carpeta `frontend`.
3. Instala las dependencias con el siguiente comando:

   ```bash
   pip install -r requirements.txt
   ```

## ConfiguraciÃ³n

1. **Token de Apify**: Necesitas un token de Apify para utilizar el scraper de Twitter. AsegÃºrate de configurar la variable de entorno `APIFY_TOKEN` antes de ejecutar el backend.

   ```bash
   export APIFY_TOKEN="tu_token_de_apify_aquÃ­"
   ```

2. **EjecuciÃ³n del Backend**: El backend debe estar en ejecuciÃ³n para que el frontend funcione correctamente. Para iniciar el servidor Flask, ejecuta:

   ```bash
   python backend.py
   ```

3. **EjecuciÃ³n del Frontend**: Una vez que el backend estÃ© en ejecuciÃ³n, puedes iniciar la aplicaciÃ³n de Streamlit con:

   ```bash
   streamlit run frontend.py
   ```

## Uso

1. **Interfaz de Usuario**: La interfaz de usuario de Streamlit permite ingresar un tÃ©rmino de bÃºsqueda, fechas de inicio y fin, y otros parÃ¡metros avanzados para filtrar los tweets.
2. **AnÃ¡lisis de Sentimientos**: DespuÃ©s de hacer clic en "Scrape Tweets", el backend realizarÃ¡ el scraping de los tweets y realizarÃ¡ un anÃ¡lisis de sentimientos bÃ¡sico.
3. **VisualizaciÃ³n de Resultados**: Los resultados se mostrarÃ¡n en la interfaz de Streamlit, incluyendo un resumen del anÃ¡lisis de sentimientos, un grÃ¡fico de distribuciÃ³n de sentimientos y ejemplos de tweets positivos y negativos.
4. **Descarga de Resultados**: Los resultados tambiÃ©n se pueden descargar en formato JSON.

## Ejemplo de Uso

1. Ingresa un tÃ©rmino de bÃºsqueda, como `#Python` o `OpenAI`.
2. Selecciona un rango de fechas.
3. Ajusta los parÃ¡metros avanzados si es necesario.
4. Haz clic en "Scrape Tweets".
5. Visualiza los resultados en la interfaz de Streamlit.


## Puntos Clave de IntegraciÃ³n

### ğŸ”§ Variables de ConfiguraciÃ³n (Revisar antes de desplegar)
backend.py
```python backend.py
DEVELOPMENT_MODE = True  # Cambiar a False para scraping real
FAKE_DATASET_PATH = "dataset_nvidia.csv"  # Datos de prueba
APIFY_TOKEN = os.getenv("APIFY_TOKEN")  # Necesario para producciÃ³n

# En frontend.py
BACKEND_URL = "http://localhost:5000/process"  # Actualizar en producciÃ³n
```

### ğŸ”„ Flujo de Datos
Frontend (Streamlit) â†’ [JSON] â†’ Backend (Flask) â†’ [Scraper/NLP] â†’ â†’ [Respuesta JSON] â†’ Frontend (VisualizaciÃ³n)


### ğŸ“¦ Estructura de ComunicaciÃ³n
**Frontend â†’ Backend (POST /process)**
```json
{
  "search_terms": "#Python",
  "start_date": "2024-01-01",
  "end_date": "2024-01-31",
  "max_items": 500,
  "tweet_language": "es",
  "min_favorites": 5
}
Backend â†’ Frontend (Respuesta)

{
  "status": "completed",
  "summary": {
    "positive_count": 120,
    "negative_count": 80
  },
  "example_tweets": {
    "positive": [{"text": "Me encanta programar!", "sentiment": "positive"}],
    "negative": [{"text": "No me gusta programar", "sentiment": "negative"}]
  }
}
```

## âš™ï¸ Funciones CrÃ­ticas
Procesamiento en Backend 

backend.py
```python
@app.route('/process', methods=['POST'])
def process_tweets():
    # 1. Recibir datos del frontend
    data = request.json
    
    # 2. Modo desarrollo vs producciÃ³n
    if DEVELOPMENT_MODE:
        df = simulate_scraping(data)  # Usa FAKE_DATASET_PATH
    else:
        df = real_scrape(data)  # Requiere APIFY_TOKEN
    
    # 3. AnÃ¡lisis de sentimientos
    df = perform_sentiment_analysis(df)
    
    # 4. Formatear respuesta
    return jsonify({
        "summary": generate_summary(df),
        "example_tweets": extract_examples(df)
    })
```

Manejo en Frontend 

frontend.py

```python
def display_results():
    # 1. Enviar solicitud al backend
    response = requests.post(BACKEND_URL, json=st.session_state.data)
    
    # 2. Procesar respuesta
    if response.status_code == 200:
        result = response.json()
        show_metrics(result['summary'])
        show_chart(result['summary'])
        show_examples(result['example_tweets'])

```
ğŸš€ Checklist para Despliegue
Modo Desarrollo:

Mantener DEVELOPMENT_MODE = True
Asegurar que dataset_nvidia.csv exista
No se requiere token de API
Modo ProducciÃ³n:

Cambiar DEVELOPMENT_MODE = False
Configurar variable de entorno APIFY_TOKEN
Actualizar BACKEND_URL si se despliega remotamente
Para Escalar:

Para muchos tweets (>1k), aumentar timeout:

backend.py

app.run(debug=True, threaded=True, timeout=600)
Considerar usar Redis para procesamiento asÃ­ncrono
ğŸ”— ConexiÃ³n con otros Componentes
# Para integrar con NLP:

Modificar:
```python
perform_sentiment_analysis()
 en 
backend.py
 para:
Llamara servicio NLP en lugar de TextBlob
Mantener la misma estructura de respuesta:
{
    "sentiment": "positive"|"negative",
    "text": "texto original"
}
```

Ejemplo de integraciÃ³n:

backend.py

```python
# Reemplazar TextBlob con modelo
def get_sentiment(text):
    # Llamar al modelo NLP aquÃ­
    return nlp_model.predict(text)
```
ğŸ“Š Estructura de Datos
Columnas requeridas en CSV para modo desarrollo:

```
author/id,createdAt,text,quoteCount,replyCount,retweetCount
```

ğŸ›  SoluciÃ³n de Problemas
Errores de conexiÃ³n: Verificar que BACKEND_URL coincida
Resultados vacÃ­os: Revisar logs del scraper
Lentitud: Reducir max_items en el formulario
